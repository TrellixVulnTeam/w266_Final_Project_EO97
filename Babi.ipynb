{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BABI_DATASET_URL = 'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'\n",
    "DATASET_PATH = './datasets/babi'\n",
    "\n",
    "def fetch_data(dataset_download_url=BABI_DATASET_URL, save_dataset_path=DATASET_PATH): \n",
    "    if not os.path.isdir(save_dataset_path):\n",
    "        os.makedirs(save_dataset_path)\n",
    "    tar_file_path = os.path.join(save_dataset_path, 'babi-tasks-v1-2.tar.gz')\n",
    "    urllib.request.urlretrieve(dataset_download_url, tar_file_path)\n",
    "    babi_tar_file = tarfile.open(tar_file_path)\n",
    "    babi_tar_file.extractall(path=DATASET_PATH)\n",
    "    babi_tar_file.close()\n",
    "    \n",
    "fetch_data(dataset_download_url=BABI_DATASET_URL, save_dataset_path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "def get_stories(train_file, only_supporting_substories=False):\n",
    "    with open(train_file) as data_file:\n",
    "        return parse_stories(data_file.readlines(), only_supporting_substories=only_supporting_substories)\n",
    "\n",
    "def parse_stories(lines, only_supporting_substories=False):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = str.lower(line)\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line: # question\n",
    "            query, answer, supporting_sentences = line.split('\\t')\n",
    "            query = tokenize(query)\n",
    "            answer = [answer]\n",
    "            substory = None\n",
    "\n",
    "            # remove question marks\n",
    "            #if query[-1] == \"?\":\n",
    "            #    query = query[:-1]\n",
    "\n",
    "            if only_supporting_substories:\n",
    "                # Only select the related substory\n",
    "                supporting_sentences = map(int, supporting_sentences.split())\n",
    "                substory = [story[i - 1] for i in supporting_sentences]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            substory = reduce((lambda x, y: x + y), substory)\n",
    "            data.append((substory, query, answer))\n",
    "            story.append('')\n",
    "        else: # regular sentence\n",
    "            # remove periods\n",
    "            sent = tokenize(line)\n",
    "            #if sent[-1] == \".\":\n",
    "            #    sent = sent[:-1]\n",
    "            story.append(sent)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['mary', 'moved', 'to', 'the', 'bathroom', '.', 'sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'mary', 'got', 'the', 'football', 'there', '.', 'john', 'went', 'to', 'the', 'kitchen', '.', 'mary', 'went', 'back', 'to', 'the', 'kitchen', '.', 'mary', 'went', 'back', 'to', 'the', 'garden', '.'], ['where', 'is', 'the', 'football', '?'], ['garden'])\n",
      "(['mary', 'got', 'the', 'milk', 'there', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'sandra', 'went', 'back', 'to', 'the', 'kitchen', '.', 'mary', 'travelled', 'to', 'the', 'hallway', '.'], ['where', 'is', 'the', 'milk', '?'], ['hallway'])\n"
     ]
    }
   ],
   "source": [
    "def get_task(data_dir, task_id, dataset='small', only_supporting_substories=False):\n",
    "    if dataset == 'small':\n",
    "        data_dir = data_dir + 'en'\n",
    "    else:\n",
    "        data_dir = data_dir + 'en-10k'\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [os.path.join(data_dir, f) for f in files]\n",
    "    file_with_task_id = 'qa{}_'.format(task_id)\n",
    "    train_file = [file_name for file_name in files if file_with_task_id in file_name and 'train' in file_name][0]\n",
    "    test_file = [file_name for file_name in files if file_with_task_id in file_name and 'test' in file_name][0]\n",
    "    train_data = get_stories(train_file, only_supporting_substories)\n",
    "    test_data = get_stories(test_file, only_supporting_substories)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "data_dir = './datasets/babi/tasks_1-20_v1-2/'\n",
    "train_data, test_data = get_task(data_dir, task_id=2, dataset='small', only_supporting_substories=False)\n",
    "print(train_data[0])\n",
    "print(test_data[0])\n",
    "# print(train_data[:2])\n",
    "# print(test_data[:2])\n",
    "#print(test_data[-2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '?', 'apple', 'back', 'bathroom', 'bedroom', 'daniel', 'discarded', 'down', 'dropped', 'football', 'garden', 'got', 'grabbed', 'hallway', 'is', 'john', 'journeyed', 'kitchen', 'left', 'mary', 'milk', 'moved', 'office', 'picked', 'put', 'sandra', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went', 'where']\n",
      "Press Enter to continue ...\n",
      "['.', '?', 'apple', 'back', 'bathroom', 'bedroom', 'daniel', 'discarded', 'down', 'dropped', 'football', 'garden', 'got', 'grabbed', 'hallway', 'is', 'john', 'journeyed', 'kitchen', 'left', 'mary', 'milk', 'moved', 'office', 'picked', 'put', 'sandra', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went', 'where']\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "#print(train_data)\n",
    "#print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "#print(test_data)\n",
    "def get_vocabulary(train_data, test_data):\n",
    "    vocabulary = set()\n",
    "    #print(vocabulary)\n",
    "    for story, query, answer in (train_data + test_data):\n",
    "        #story = reduce((lambda x, y: x + y), story)\n",
    "        vocabulary |= set(story + query  + answer)\n",
    "    return sorted(vocabulary)\n",
    "vocabulary = get_vocabulary(train_data, test_data)\n",
    "print(vocabulary)\n",
    "raw_input(\"Press Enter to continue ...\")\n",
    "print(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apple': 3, 'office': 24, 'is': 16, 'moved': 23, 'back': 4, 'down': 9, 'dropped': 10, 'picked': 25, 'bedroom': 6, 'milk': 22, 'bathroom': 5, 'grabbed': 14, 'there': 29, '.': 1, 'to': 30, 'daniel': 7, 'got': 13, 'travelled': 32, 'john': 17, 'mary': 21, '?': 2, 'hallway': 15, 'garden': 12, 'football': 11, 'took': 31, 'sandra': 27, 'put': 26, 'went': 34, 'kitchen': 19, 'journeyed': 18, 'up': 33, 'discarded': 8, 'the': 28, 'where': 35, 'left': 20}\n"
     ]
    }
   ],
   "source": [
    "def get_word_index(vocabulary):\n",
    "    return dict((c, i+1) for i, c in enumerate(vocabulary))\n",
    "word_indices = get_word_index(vocabulary)\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def get_story_max_length(train_data, test_data):\n",
    "    return max(map(len, (story for story, _, _ in (train_data + test_data) )))\n",
    "\n",
    "def get_query_max_length(train_data, test_data):\n",
    "    return max(map(len, (query for _, query, _ in (train_data + test_data) )))\n",
    "\n",
    "def get_answer_max_length(train_data, test_data):\n",
    "    return max(map(len, (answer for _, _, answer in (train_data + test_data) )))\n",
    "\n",
    "print(get_story_max_length(train_data, test_data))\n",
    "print(get_query_max_length(train_data, test_data))\n",
    "print(get_answer_max_length(train_data, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(train_data, test_data)\n",
    "word_indices = get_word_index(vocabulary)\n",
    "story_max_length = get_story_max_length(train_data, test_data)\n",
    "query_max_length = get_query_max_length(train_data, test_data)\n",
    "vocabulary_size = len(vocabulary) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_indices, story_max_length, query_max_length):\n",
    "    S = []\n",
    "    Q = []\n",
    "    A = []\n",
    "    for story, query, answer in data:\n",
    "        ls = max(0, story_max_length - len(story))\n",
    "        x = [word_indices[w] for w in story] + [0] * ls\n",
    "        lq = max(0, query_max_length - len(query))\n",
    "        xq = [word_indices[w] for w in query] + [0] * lq\n",
    "        y = np.zeros(len(word_indices) + 1)\n",
    "        answer = \"%s\" % \"','\".join(answer)\n",
    "        y[word_indices[answer]] = 1\n",
    "        S.append(x)\n",
    "        Q.append(xq)\n",
    "        A.append(y)\n",
    "    return np.array(S), np.array(Q), np.array(A)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27 25 33 28 11 29  1  7 23 30 28  5  1 27 32 30 28  5  1 21 32 30 28 12  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "[35 16 28 11  2]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "train_story, train_query, train_answer = vectorize_stories(train_data, word_indices, \n",
    "                                                           story_max_length, query_max_length)\n",
    "test_story, test_query, test_answer = vectorize_stories(test_data, word_indices, \n",
    "                                                           story_max_length, query_max_length)\n",
    "\n",
    "print(train_story[100])\n",
    "print(train_query[100])\n",
    "print(train_answer[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['.', '?', 'apple', 'back', 'bathroom', 'bedroom', 'daniel', 'discarded', 'down', 'dropped', 'football', 'garden', 'got', 'grabbed', 'hallway', 'is', 'john', 'journeyed', 'kitchen', 'left', 'mary', 'milk', 'moved', 'office', 'picked', 'put', 'sandra', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went', 'where']\n",
      "vocab length = 35\n",
      "word_indices length = 35\n",
      "train_story.shape = (1000, 552)\n",
      "train_query.shape = (1000, 5)\n",
      "train_answer.shape = (1000, 36)\n",
      "story_maxlen, query_maxlen = 552, 5\n"
     ]
    }
   ],
   "source": [
    "print('vocab = {}'.format(vocabulary))\n",
    "print('vocab length = {}'.format(len(vocabulary)))\n",
    "print('word_indices length = {}'.format(len(word_indices)))\n",
    "print('train_story.shape = {}'.format(train_story.shape))\n",
    "print('train_query.shape = {}'.format(train_query.shape))\n",
    "print('train_answer.shape = {}'.format(train_answer.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_max_length, query_max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
