{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import tarfile\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "import os\n",
    "from six.moves import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BABI_DATASET_URL = 'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'\n",
    "DATASET_PATH = './babi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_data(dataset_download_url=BABI_DATASET_URL, save_dataset_path=DATASET_PATH): \n",
    "    if not os.path.isdir(save_dataset_path):\n",
    "        os.makedirs(save_dataset_path)\n",
    "    tar_file_path = os.path.join(save_dataset_path, 'babi_tasks_data_1_20_v1.2.tar.gz')\n",
    "    urllib.request.urlretrieve(dataset_download_url, tar_file_path)\n",
    "    babi_tar_file = tarfile.open(tar_file_path)\n",
    "    babi_tar_file.extractall(path=DATASET_PATH)\n",
    "    babi_tar_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch_data(dataset_download_url=BABI_DATASET_URL, save_dataset_path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    Model objects are passed a Config() object at instantiation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    batch_size = 32\n",
    "    embed_size = 50\n",
    "    hidden_size = 50\n",
    "    #max_epochs = 5000\n",
    "    max_epochs = 500\n",
    "    dropout = 0.7\n",
    "    lr = 0.8\n",
    "    L2 = 0.001\n",
    "\n",
    "    vocab_size = None\n",
    "    num_steps_sentence = None\n",
    "    num_steps_story = None\n",
    "    num_steps_question = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_Model:\n",
    "    def __init__(self, config):\n",
    "        self.config = config    \n",
    "        self.add_placeholders()\n",
    "        self.inputs_story, self.inputs_question = self.add_embedding()\n",
    "        self.story_question_state = self.add_basic_model(self.inputs_story, self.inputs_question)\n",
    "\n",
    "        self.output = self.add_projection(self.story_question_state)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            self.predictions = tf.nn.softmax(self.output)\n",
    "            self.one_hot_prediction = tf.argmax(self.predictions, 1)\n",
    "            correct_prediction = tf.equal(tf.argmax(self.labels_placeholder, 1), self.one_hot_prediction)\n",
    "            self.correct_predictions = tf.reduce_sum(tf.cast(correct_prediction, 'int32'))\n",
    "\n",
    "        with tf.name_scope('Loss'):\n",
    "            loss, lossL2 = self.add_loss_op(self.output)\n",
    "            self.calculate_loss = loss + self.config.L2 * lossL2\n",
    "        with tf.name_scope('Train'):\n",
    "            self.train_step = self.add_training_op(self.calculate_loss)\n",
    "\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors\n",
    "        \"\"\"\n",
    "        self.input_story_placeholder = tf.placeholder(\n",
    "            tf.int32, shape=[None, self.config.num_steps_story, self.config.num_steps_sentence], name='InputStory')\n",
    "        self.input_question_placeholder = tf.placeholder(\n",
    "            tf.int32, shape=[None, self.config.num_steps_question], name='InputQuestion')\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            tf.int32, shape=[None, self.config.vocab_size], name='Target')\n",
    "        self.X_length = tf.placeholder(tf.int32, shape=[None], name='X_length')\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32, name='Dropout')\n",
    "\n",
    "        \n",
    "    def add_embedding(self):\n",
    "        \"\"\"Add embedding layer.\n",
    "\n",
    "        Returns:\n",
    "          inputs: List of length num_steps, each of whose elements should be\n",
    "                  a tensor of shape (batch_size, embed_size).\n",
    "        \"\"\"\n",
    "        embedding = tf.get_variable('Embedding', [self.config.vocab_size, self.config.embed_size], trainable=True,\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "        inputs_story = tf.nn.embedding_lookup(embedding, self.input_story_placeholder)\n",
    "        inputs_question = tf.nn.embedding_lookup(embedding, self.input_question_placeholder)\n",
    "\n",
    "        # Position Encoding\n",
    "        inputs_question = tf.expand_dims(inputs_question, 1)\n",
    "        encoded_story = self.get_position_encoding(inputs_story, self.config.num_steps_sentence, 'StoryEncoding')\n",
    "        encoded_query = self.get_position_encoding(inputs_question, self.config.num_steps_question, 'QueryEncoding')\n",
    "        encoded_query = tf.tile(encoded_query, tf.stack([1, self.config.num_steps_story, 1]), name=None)\n",
    "\n",
    "        return encoded_story, encoded_query\n",
    "\n",
    "    def get_position_encoding(self, embedding, max_length, scope=None):\n",
    "        \"\"\"\n",
    "        Module described in [End-To-End Memory Networks](https://arxiv.org/abs/1502.01852) as Position Encoding (PE).\n",
    "        The mask allows the ordering of words in a sentence to affect the encoding.\n",
    "        \"\"\"\n",
    "        J, d = max_length, self.config.embed_size\n",
    "        l = np.zeros((J, d))\n",
    "        with tf.variable_scope(scope, 'PE'):\n",
    "            for j in range(J):\n",
    "                for k in range(d):\n",
    "                    l[j, k] = (1. - (j + 1.) / J) - ((k + 1.) / d) * (1. - 2. * (j + 1.) / J)\n",
    "            self.l = tf.constant(l, shape=[J, d])\n",
    "            m = tf.reduce_sum(embedding * tf.cast(self.l, tf.float32), 2, name='m')\n",
    "        return m\n",
    "\n",
    "    def add_projection(self, rnn_output):\n",
    "        \"\"\"Adds a projection layer.\n",
    "\n",
    "        The projection layer transforms the hidden representation to a distribution\n",
    "        over the vocabulary.\n",
    "\n",
    "        Args:\n",
    "          rnn_output: List of length num_steps, each of whose elements should be\n",
    "                       a tensor of shape (batch_size, embed_size).\n",
    "        Returns:\n",
    "          outputs: List of length num_steps, each a tensor of shape\n",
    "                   (batch_size, len(vocab))\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('Projection'):\n",
    "            U = tf.get_variable('Weights',\n",
    "                                [self.config.hidden_size, self.config.vocab_size], trainable=True,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('Bias', [self.config.vocab_size])\n",
    "            outputs = tf.matmul(rnn_output, U) + b\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def add_loss_op(self, output):\n",
    "        \"\"\"Adds loss ops to the computational graph.\n",
    "\n",
    "        Args:\n",
    "          output: A tensor of shape (None, self.vocab)\n",
    "        Returns:\n",
    "          loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        var = tf.trainable_variables()\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.labels_placeholder))\n",
    "        tf.add_to_collection('total_loss', cross_entropy)\n",
    "        loss = tf.add_n(tf.get_collection('total_loss'))\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in var if 'Bias' not in v.name])\n",
    "\n",
    "        return loss, lossL2\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "        Args:\n",
    "          loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "          train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        optimizer = tf.train.AdagradOptimizer(self.config.lr)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def add_basic_model(self, inputs_story, inputs_question):\n",
    "        basic_lstm = tf.contrib.rnn.BasicLSTMCell(num_units=self.config.hidden_size)\n",
    "        basic_lstm = tf.contrib.rnn.DropoutWrapper(basic_lstm, input_keep_prob=self.dropout_placeholder,\n",
    "                                            output_keep_prob=self.dropout_placeholder)\n",
    "\n",
    "        with tf.variable_scope('basic_cell') as scope:\n",
    "            #a, b = basic_dynamic_rnn(basic_lstm, [inputs_story, inputs_question],\n",
    "            #                                sequence_length=self.X_length,\n",
    "            #                                dtype=tf.float32)\n",
    "            a, b = basic_dynamic_rnn(basic_lstm, [inputs_story, inputs_question],\n",
    "                                            sequence_length=self.X_length,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "        return tf.reduce_sum(b, 0)\n",
    "        \n",
    "    \n",
    "\n",
    "    def predict(self, session, data):\n",
    "        input_story, input_question, input_labels, X_length = data\n",
    "        config = self.config\n",
    "        dp = 1\n",
    "\n",
    "        n_data = len(input_story)\n",
    "        batches = zip(range(0, n_data - config.batch_size, config.batch_size),\n",
    "                      range(config.batch_size, n_data, config.batch_size))\n",
    "        batches = [(start, end) for start, end in batches]\n",
    "        total_correct_examples = 0\n",
    "        total_processed_examples = 0\n",
    "        for step, (start, end) in enumerate(batches):\n",
    "            feed = {self.input_story_placeholder: input_story[start:end],\n",
    "                    self.input_question_placeholder: input_question[start:end],\n",
    "                    self.labels_placeholder: input_labels[start:end],\n",
    "                    self.dropout_placeholder: dp,\n",
    "                    self.X_length: X_length[start:end]}\n",
    "            total_correct = session.run(self.correct_predictions, feed_dict=feed)\n",
    "            total_processed_examples += end - start\n",
    "            total_correct_examples += total_correct\n",
    "        acc = total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "        return acc\n",
    "\n",
    "    \n",
    "    def run_epoch(self, session, data, train_op=None, verbose=10):\n",
    "            input_story, input_question, input_labels, X_length = data\n",
    "            config = self.config\n",
    "            dp = config.dropout\n",
    "\n",
    "            if not train_op:\n",
    "                train_op = tf.no_op()\n",
    "                dp = 1\n",
    "\n",
    "            n_data = len(input_story)\n",
    "            batches = zip(range(0, n_data - config.batch_size, config.batch_size),\n",
    "                          range(config.batch_size, n_data, config.batch_size))\n",
    "            batches = [(start, end) for start, end in batches]\n",
    "            np.random.shuffle(batches)\n",
    "            n_val = int(len(batches) * 0.1)\n",
    "            batches_train = batches[:-n_val]\n",
    "            batches_val = batches[-n_val:]\n",
    "\n",
    "            total_loss = []\n",
    "            total_correct_examples = 0\n",
    "            total_processed_examples = 0\n",
    "            total_steps = len(batches_train)\n",
    "            for step, (start, end) in enumerate(batches_train):\n",
    "                feed = {self.input_story_placeholder: input_story[start:end],\n",
    "                        self.input_question_placeholder: input_question[start:end],\n",
    "                        self.labels_placeholder: input_labels[start:end],\n",
    "                        self.dropout_placeholder: dp,\n",
    "                        self.X_length: X_length[start:end]}\n",
    "                loss, total_correct, _ = session.run(\n",
    "                    [self.calculate_loss, self.correct_predictions, train_op],\n",
    "                    feed_dict=feed)\n",
    "                total_processed_examples += end - start\n",
    "                total_correct_examples += total_correct\n",
    "                total_loss.append(loss)\n",
    "                if verbose and step % verbose == 0:\n",
    "                    sys.stdout.write('\\r{} / {} : loss = {}'.format(\n",
    "                        step, total_steps, np.mean(total_loss)))\n",
    "                    sys.stdout.flush()\n",
    "                if verbose:\n",
    "                    sys.stdout.write('\\r')\n",
    "            train_acc = total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "            Story = []\n",
    "            Question = []\n",
    "            Answer = []\n",
    "            Prediction = []\n",
    "            total_correct_examples = 0\n",
    "            total_processed_examples = 0\n",
    "            for step, (start, end) in enumerate(batches_val):\n",
    "                feed = {self.input_story_placeholder: input_story[start:end],\n",
    "                        self.input_question_placeholder: input_question[start:end],\n",
    "                        self.labels_placeholder: input_labels[start:end],\n",
    "                        self.dropout_placeholder: 1,\n",
    "                        self.X_length: X_length[start:end]}\n",
    "                total_correct, prediction = session.run([self.correct_predictions, self.one_hot_prediction], feed_dict=feed)\n",
    "                total_processed_examples += end - start\n",
    "                total_correct_examples += total_correct\n",
    "\n",
    "                Story.append(input_story[start:end])\n",
    "                Question.append(input_question[start:end])\n",
    "                Answer.append(input_labels[start:end])\n",
    "                Prediction.append(prediction)\n",
    "\n",
    "            val_acc = total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "            return np.mean(total_loss), train_acc, val_acc, Story, Question, Answer, Prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_dynamic_rnn(cell_fw, inputs, sequence_length=None,\n",
    "                                     initial_state_fw=None, \n",
    "                                     dtype=None, parallel_iterations=None,\n",
    "                                     swap_memory=False, time_major=False, scope=None):\n",
    "    with vs.variable_scope(scope or \"unidirectional_rnn\"):\n",
    "        \n",
    "        with vs.variable_scope(\"fw\") as fw_scope:\n",
    "            inputs_concat = tf.concat(inputs, 1)\n",
    "            output_fw, output_state_fw = tf.nn.dynamic_rnn(\n",
    "                cell=cell_fw, inputs=inputs_concat, sequence_length=sequence_length,\n",
    "                initial_state=initial_state_fw, dtype=dtype\n",
    "                )\n",
    "            \n",
    "    return output_fw, output_state_fw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=3242):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def get_data(task_path):\n",
    "    train = get_stories(tar.extractfile(task_path.format('train')))\n",
    "    test = get_stories(tar.extractfile(task_path.format('test')))\n",
    "    return train, test\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    \"\"\"Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    \"\"\"\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    data = [(story, q, answer) for story, q, answer in data if\n",
    "            not max_length or len(story) < max_length]\n",
    "    return data\n",
    "    \n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    \"\"\"\n",
    "    Parse the bAbI task format.\n",
    "    If only_supporting is True, only the sentences that support the answer are kept.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize_word(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize_word(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize_word(sent):\n",
    "    \"\"\"Return the tokens of a sentence excluding punctuation.\n",
    "    >> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', 'Bob', 'went', 'to', 'the', 'kitchen']\n",
    "    \"\"\"\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, sentence_maxlen, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    X_length = []\n",
    "\n",
    "    for story, query, answer in data:\n",
    "        sentences = []\n",
    "        for s in story:\n",
    "            sentence = [word_idx[w] for w in s]\n",
    "            for _ in range(sentence_maxlen - len(sentence)):\n",
    "                sentence.append(0)\n",
    "            assert len(sentence) == sentence_maxlen\n",
    "            sentences.append(sentence)\n",
    "        X_length.append(len(sentences))\n",
    "\n",
    "        ## story\n",
    "        for _ in range(story_maxlen - len(sentences)):\n",
    "            sentences.append([0 for _ in range(sentence_maxlen)])\n",
    "\n",
    "        ## query\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        for _ in range(query_maxlen - len(xq)):\n",
    "            xq.append(0)\n",
    "        ## answer\n",
    "        y = np.zeros(len(word_idx) + 1)  # index 0 is reserved\n",
    "        y[word_idx[answer]] = 1\n",
    "\n",
    "        X.append(sentences)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "\n",
    "    return X, Xq, np.array(Y), X_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************\n",
      "\n",
      "qa19_path-finding\n",
      "vocab = [u'.', u'?', u'How', u'The', u'bathroom', u'bedroom', u'do', u'e,e', u'e,n', u'e,s', u'east', u'from', u'garden', u'go', u'hallway', u'is', u'kitchen', u'n,e', u'n,n', u'n,w', u'north', u'of', u'office', u's,e', u's,s', u's,w', u'south', u'the', u'to', u'w,n', u'w,s', u'w,w', u'west', u'you']\n",
      "word_idx = {u'w,s': 31, u'office': 23, u'hallway': 15, u'is': 16, u'How': 3, u'w,w': 32, u'bedroom': 6, u'go': 14, u's,s': 25, u'bathroom': 5, u'from': 12, u'west': 33, u's,w': 26, u'.': 1, u'to': 29, u'w,n': 30, u's,e': 24, u'you': 34, u'east': 11, u'?': 2, u'do': 7, u'north': 21, u'garden': 13, u'n,w': 20, u'kitchen': 17, u'n,e': 18, u'The': 4, u'n,n': 19, u'e,n': 9, u'of': 22, u'e,e': 8, u'e,s': 10, u'the': 28, u'south': 27}\n",
      "X.shape = (1000, 5, 8)\n",
      "Xq.shape = (1000, 11)\n",
      "Y.shape = (1000, 35)\n",
      "story_maxlen, sentence_maxlen, query_maxlen = 5, 8, 11\n",
      "max_epochs = 500\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name bayesflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7b03d3170ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_epochs = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e7cdce3235f2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstory_question_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_basic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e7cdce3235f2>\u001b[0m in \u001b[0;36madd_embedding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \"\"\"\n\u001b[1;32m     43\u001b[0m         embedding = tf.get_variable('Embedding', [self.config.vocab_size, self.config.embed_size], trainable=True,\n\u001b[0;32m---> 44\u001b[0;31m                                     initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0minputs_story\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_story_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0minputs_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_question_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shankarz/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shankarz/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.pyc\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shankarz/anaconda2/lib/python2.7/importlib/__init__.pyc\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resolve_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shankarz/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Add projects here, they will show up under tf.contrib.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbayesflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name bayesflow"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# tasks = [\n",
    "#     'qa1_single-supporting-fact', 'qa2_two-supporting-facts', 'qa3_three-supporting-facts',\n",
    "#     'qa4_two-arg-relations', 'qa5_three-arg-relations', 'qa6_yes-no-questions', 'qa7_counting',\n",
    "#     'qa8_lists-sets', 'qa9_simple-negation', 'qa10_indefinite-knowledge',\n",
    "#     'qa11_basic-coreference', 'qa12_conjunction', 'qa13_compound-coreference',\n",
    "#     'qa14_time-reasoning', 'qa15_basic-deduction', 'qa16_basic-induction', 'qa17_positional-reasoning',\n",
    "#     'qa18_size-reasoning', 'qa19_path-finding', 'qa20_agents-motivations'\n",
    "# ]\n",
    "\n",
    "tasks = ['qa19_path-finding', 'qa20_agents-motivations']\n",
    "\n",
    "verbose = True\n",
    "path = 'babi/babi_tasks_data_1_20_v1.2.tar.gz'\n",
    "tar = tarfile.open(path)\n",
    "tasks_dir = 'tasks_1-20_v1-2/en/'\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "    print('\\n**************************************\\n')\n",
    "    print(task)\n",
    "    \n",
    "    task_path = tasks_dir + task + '_{}.txt'\n",
    "    train, test = get_data(task_path)\n",
    "    \n",
    "    flatten = lambda data: [i for i in chain(*data)]\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(flatten(story) + q + [answer])\n",
    "                                               for story, q, answer in train + test)))\n",
    "\n",
    "    # 0 is reserved for masking via pad_sequences\n",
    "    vocab_size = len(vocab) + 1\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "    sentence_maxlen = max(flatten([[len(s) for s in x] for x, _, _ in train + test]))\n",
    "    #sentence_maxlen = max([i for i in chain(*[[len(s) for s in x] for x, _, _ in train + test])])\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "    \n",
    "    idx_word = {v: k for k, v in word_idx.iteritems()}\n",
    "    idx_word[0] = \"_PAD\"\n",
    "     \n",
    "    X, Xq, Y, X_length = vectorize_stories(train, word_idx, sentence_maxlen, story_maxlen, query_maxlen)\n",
    "    tX, tXq, tY, tX_length = vectorize_stories(test, word_idx, sentence_maxlen, story_maxlen, query_maxlen)\n",
    "    \n",
    "    if verbose:\n",
    "        print('vocab = {}'.format(vocab))\n",
    "        print('word_idx = {}'.format(word_idx))\n",
    "        print('X.shape = {}'.format(np.array(X).shape))\n",
    "        print('Xq.shape = {}'.format(np.array(Xq).shape))\n",
    "        print('Y.shape = {}'.format(Y.shape))\n",
    "        print('story_maxlen, sentence_maxlen, query_maxlen = {}, {}, {}'.format(story_maxlen, sentence_maxlen, query_maxlen))\n",
    "\n",
    "    config = Config()\n",
    "    config.vocab_size = vocab_size\n",
    "    config.num_steps_sentence = sentence_maxlen\n",
    "    config.num_steps_story = story_maxlen\n",
    "    config.num_steps_question = query_maxlen\n",
    "    print('max_epochs = {}'.format(config.max_epochs)) \n",
    "    with tf.Graph().as_default() as g:\n",
    "        model = RNN_Model(config)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "\n",
    "            for epoch in range(config.max_epochs):\n",
    "                print('Epoch {}'.format(epoch))\n",
    "\n",
    "                train_loss, train_acc, val_acc, Story, Question, Answer, Prediction = model.run_epoch(session, (\n",
    "                    X, Xq, Y, X_length), train_op=model.train_step)\n",
    "\n",
    "                if verbose:\n",
    "                    print('Training loss: {}'.format(train_loss))\n",
    "                    print('Training acc: {}'.format(train_acc))\n",
    "                    print('Validation acc: {}'.format(val_acc))\n",
    "\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    test_acc = model.predict(session, (tX, tXq, tY, tX_length))\n",
    "                    print('Testing acc: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
