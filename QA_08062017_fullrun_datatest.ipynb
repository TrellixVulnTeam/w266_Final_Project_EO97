{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import tarfile\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "import os\n",
    "from six.moves import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BABI_DATASET_URL = 'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'\n",
    "DATASET_PATH = './babi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_data(dataset_download_url=BABI_DATASET_URL, save_dataset_path=DATASET_PATH): \n",
    "    if not os.path.isdir(save_dataset_path):\n",
    "        os.makedirs(save_dataset_path)\n",
    "    tar_file_path = os.path.join(save_dataset_path, 'babi_tasks_data_1_20_v1.2.tar.gz')\n",
    "    urllib.request.urlretrieve(dataset_download_url, tar_file_path)\n",
    "    babi_tar_file = tarfile.open(tar_file_path)\n",
    "    babi_tar_file.extractall(path=DATASET_PATH)\n",
    "    babi_tar_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch_data(dataset_download_url=BABI_DATASET_URL, save_dataset_path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    Model objects are passed a Config() object at instantiation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    batch_size = 32\n",
    "    embed_size = 50\n",
    "    hidden_size = 50\n",
    "    #max_epochs = 5000\n",
    "    max_epochs = 500\n",
    "    dropout = 0.7\n",
    "    lr = 0.8\n",
    "    L2 = 0.001\n",
    "\n",
    "    vocab_size = None\n",
    "    num_steps_sentence = None\n",
    "    num_steps_story = None\n",
    "    num_steps_question = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_Model:\n",
    "    def __init__(self, config):\n",
    "        self.config = config    \n",
    "        self.add_placeholders()\n",
    "        self.inputs_story, self.inputs_question = self.add_embedding()\n",
    "        self.story_question_state = self.add_basic_model(self.inputs_story, self.inputs_question)\n",
    "\n",
    "        self.output = self.add_projection(self.story_question_state)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            self.predictions = tf.nn.softmax(self.output)\n",
    "            self.one_hot_prediction = tf.argmax(self.predictions, 1)\n",
    "            correct_prediction = tf.equal(tf.argmax(self.labels_placeholder, 1), self.one_hot_prediction)\n",
    "            self.correct_predictions = tf.reduce_sum(tf.cast(correct_prediction, 'int32'))\n",
    "\n",
    "        with tf.name_scope('Loss'):\n",
    "            loss, lossL2 = self.add_loss_op(self.output)\n",
    "            self.calculate_loss = loss + self.config.L2 * lossL2\n",
    "        with tf.name_scope('Train'):\n",
    "            self.train_step = self.add_training_op(self.calculate_loss)\n",
    "\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors\n",
    "        \"\"\"\n",
    "        self.input_story_placeholder = tf.placeholder(\n",
    "            tf.int32, shape=[None, self.config.num_steps_story, self.config.num_steps_sentence], name='InputStory')\n",
    "        self.input_question_placeholder = tf.placeholder(\n",
    "            tf.int32, shape=[None, self.config.num_steps_question], name='InputQuestion')\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            tf.int32, shape=[None, self.config.vocab_size], name='Target')\n",
    "        self.X_length = tf.placeholder(tf.int32, shape=[None], name='X_length')\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32, name='Dropout')\n",
    "\n",
    "        \n",
    "    def add_embedding(self):\n",
    "        \"\"\"Add embedding layer.\n",
    "\n",
    "        Returns:\n",
    "          inputs: List of length num_steps, each of whose elements should be\n",
    "                  a tensor of shape (batch_size, embed_size).\n",
    "        \"\"\"\n",
    "        embedding = tf.get_variable('Embedding', [self.config.vocab_size, self.config.embed_size], trainable=True,\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "        inputs_story = tf.nn.embedding_lookup(embedding, self.input_story_placeholder)\n",
    "        inputs_question = tf.nn.embedding_lookup(embedding, self.input_question_placeholder)\n",
    "\n",
    "        # Position Encoding\n",
    "        inputs_question = tf.expand_dims(inputs_question, 1)\n",
    "        encoded_story = self.get_position_encoding(inputs_story, self.config.num_steps_sentence, 'StoryEncoding')\n",
    "        encoded_query = self.get_position_encoding(inputs_question, self.config.num_steps_question, 'QueryEncoding')\n",
    "        encoded_query = tf.tile(encoded_query, tf.stack([1, self.config.num_steps_story, 1]), name=None)\n",
    "\n",
    "        return encoded_story, encoded_query\n",
    "\n",
    "    def get_position_encoding(self, embedding, max_length, scope=None):\n",
    "        \"\"\"\n",
    "        Module described in [End-To-End Memory Networks](https://arxiv.org/abs/1502.01852) as Position Encoding (PE).\n",
    "        The mask allows the ordering of words in a sentence to affect the encoding.\n",
    "        \"\"\"\n",
    "        J, d = max_length, self.config.embed_size\n",
    "        l = np.zeros((J, d))\n",
    "        with tf.variable_scope(scope, 'PE'):\n",
    "            for j in range(J):\n",
    "                for k in range(d):\n",
    "                    l[j, k] = (1. - (j + 1.) / J) - ((k + 1.) / d) * (1. - 2. * (j + 1.) / J)\n",
    "            self.l = tf.constant(l, shape=[J, d])\n",
    "            m = tf.reduce_sum(embedding * tf.cast(self.l, tf.float32), 2, name='m')\n",
    "        return m\n",
    "\n",
    "    def add_projection(self, rnn_output):\n",
    "        \"\"\"Adds a projection layer.\n",
    "\n",
    "        The projection layer transforms the hidden representation to a distribution\n",
    "        over the vocabulary.\n",
    "\n",
    "        Args:\n",
    "          rnn_output: List of length num_steps, each of whose elements should be\n",
    "                       a tensor of shape (batch_size, embed_size).\n",
    "        Returns:\n",
    "          outputs: List of length num_steps, each a tensor of shape\n",
    "                   (batch_size, len(vocab))\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('Projection'):\n",
    "            U = tf.get_variable('Weights',\n",
    "                                [self.config.hidden_size, self.config.vocab_size], trainable=True,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('Bias', [self.config.vocab_size])\n",
    "            outputs = tf.matmul(rnn_output, U) + b\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def add_loss_op(self, output):\n",
    "        \"\"\"Adds loss ops to the computational graph.\n",
    "\n",
    "        Args:\n",
    "          output: A tensor of shape (None, self.vocab)\n",
    "        Returns:\n",
    "          loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        var = tf.trainable_variables()\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.labels_placeholder))\n",
    "        tf.add_to_collection('total_loss', cross_entropy)\n",
    "        loss = tf.add_n(tf.get_collection('total_loss'))\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in var if 'Bias' not in v.name])\n",
    "\n",
    "        return loss, lossL2\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "        Args:\n",
    "          loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "          train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        optimizer = tf.train.AdagradOptimizer(self.config.lr)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def add_basic_model(self, inputs_story, inputs_question):\n",
    "        basic_lstm = tf.contrib.rnn.BasicLSTMCell(num_units=self.config.hidden_size)\n",
    "        basic_lstm = tf.contrib.rnn.DropoutWrapper(basic_lstm, input_keep_prob=self.dropout_placeholder,\n",
    "                                            output_keep_prob=self.dropout_placeholder)\n",
    "\n",
    "        with tf.variable_scope('basic_cell') as scope:\n",
    "            #a, b = basic_dynamic_rnn(basic_lstm, [inputs_story, inputs_question],\n",
    "            #                                sequence_length=self.X_length,\n",
    "            #                                dtype=tf.float32)\n",
    "            a, b = basic_dynamic_rnn(basic_lstm, [inputs_story, inputs_question],\n",
    "                                            sequence_length=self.X_length,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "        return tf.reduce_sum(b, 0)\n",
    "        \n",
    "    \n",
    "\n",
    "    def predict(self, session, data):\n",
    "        input_story, input_question, input_labels, X_length = data\n",
    "        config = self.config\n",
    "        dp = 1\n",
    "\n",
    "        n_data = len(input_story)\n",
    "        batches = zip(range(0, n_data - config.batch_size, config.batch_size),\n",
    "                      range(config.batch_size, n_data, config.batch_size))\n",
    "        batches = [(start, end) for start, end in batches]\n",
    "        total_correct_examples = 0\n",
    "        total_processed_examples = 0\n",
    "        for step, (start, end) in enumerate(batches):\n",
    "            feed = {self.input_story_placeholder: input_story[start:end],\n",
    "                    self.input_question_placeholder: input_question[start:end],\n",
    "                    self.labels_placeholder: input_labels[start:end],\n",
    "                    self.dropout_placeholder: dp,\n",
    "                    self.X_length: X_length[start:end]}\n",
    "            total_correct = session.run(self.correct_predictions, feed_dict=feed)\n",
    "            total_processed_examples += end - start\n",
    "            total_correct_examples += total_correct\n",
    "        acc = total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "        return acc\n",
    "\n",
    "    \n",
    "    def run_epoch(self, session, data, train_op=None, verbose=10):\n",
    "            input_story, input_question, input_labels, X_length = data\n",
    "            config = self.config\n",
    "            dp = config.dropout\n",
    "\n",
    "            if not train_op:\n",
    "                train_op = tf.no_op()\n",
    "                dp = 1\n",
    "\n",
    "            n_data = len(input_story)\n",
    "            batches = zip(range(0, n_data - config.batch_size, config.batch_size),\n",
    "                          range(config.batch_size, n_data, config.batch_size))\n",
    "            batches = [(start, end) for start, end in batches]\n",
    "            np.random.shuffle(batches)\n",
    "            n_val = int(len(batches) * 0.1)\n",
    "            batches_train = batches[:-n_val]\n",
    "            batches_val = batches[-n_val:]\n",
    "\n",
    "            total_loss = []\n",
    "            total_correct_examples = 0\n",
    "            total_processed_examples = 0\n",
    "            total_steps = len(batches_train)\n",
    "            for step, (start, end) in enumerate(batches_train):\n",
    "                feed = {self.input_story_placeholder: input_story[start:end],\n",
    "                        self.input_question_placeholder: input_question[start:end],\n",
    "                        self.labels_placeholder: input_labels[start:end],\n",
    "                        self.dropout_placeholder: dp,\n",
    "                        self.X_length: X_length[start:end]}\n",
    "                loss, total_correct, _ = session.run(\n",
    "                    [self.calculate_loss, self.correct_predictions, train_op],\n",
    "                    feed_dict=feed)\n",
    "                total_processed_examples += end - start\n",
    "                total_correct_examples += total_correct\n",
    "                total_loss.append(loss)\n",
    "                if verbose and step % verbose == 0:\n",
    "                    sys.stdout.write('\\r{} / {} : loss = {}'.format(\n",
    "                        step, total_steps, np.mean(total_loss)))\n",
    "                    sys.stdout.flush()\n",
    "                if verbose:\n",
    "                    sys.stdout.write('\\r')\n",
    "            train_acc = total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "            Story = []\n",
    "            Question = []\n",
    "            Answer = []\n",
    "            Prediction = []\n",
    "            total_correct_examples = 0\n",
    "            total_processed_examples = 0\n",
    "            for step, (start, end) in enumerate(batches_val):\n",
    "                feed = {self.input_story_placeholder: input_story[start:end],\n",
    "                        self.input_question_placeholder: input_question[start:end],\n",
    "                        self.labels_placeholder: input_labels[start:end],\n",
    "                        self.dropout_placeholder: 1,\n",
    "                        self.X_length: X_length[start:end]}\n",
    "                total_correct, prediction = session.run([self.correct_predictions, self.one_hot_prediction], feed_dict=feed)\n",
    "                total_processed_examples += end - start\n",
    "                total_correct_examples += total_correct\n",
    "\n",
    "                Story.append(input_story[start:end])\n",
    "                Question.append(input_question[start:end])\n",
    "                Answer.append(input_labels[start:end])\n",
    "                Prediction.append(prediction)\n",
    "\n",
    "            val_acc = total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "            return np.mean(total_loss), train_acc, val_acc, Story, Question, Answer, Prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_dynamic_rnn(cell_fw, inputs, sequence_length=None,\n",
    "                                     initial_state_fw=None, \n",
    "                                     dtype=None, parallel_iterations=None,\n",
    "                                     swap_memory=False, time_major=False, scope=None):\n",
    "    with vs.variable_scope(scope or \"unidirectional_rnn\"):\n",
    "        \n",
    "        with vs.variable_scope(\"fw\") as fw_scope:\n",
    "            inputs_concat = tf.concat(inputs, 1)\n",
    "            output_fw, output_state_fw = tf.nn.dynamic_rnn(\n",
    "                cell=cell_fw, inputs=inputs_concat, sequence_length=sequence_length,\n",
    "                initial_state=initial_state_fw, dtype=dtype\n",
    "                )\n",
    "            \n",
    "    return output_fw, output_state_fw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=3242):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def get_data(task_path):\n",
    "    train = get_stories(tar.extractfile(task_path.format('train')))\n",
    "    test = get_stories(tar.extractfile(task_path.format('test')))\n",
    "    return train, test\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    \"\"\"Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    \"\"\"\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    data = [(story, q, answer) for story, q, answer in data if\n",
    "            not max_length or len(story) < max_length]\n",
    "    return data\n",
    "    \n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    \"\"\"\n",
    "    Parse the bAbI task format.\n",
    "    If only_supporting is True, only the sentences that support the answer are kept.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize_word(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize_word(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize_word(sent):\n",
    "    \"\"\"Return the tokens of a sentence excluding punctuation.\n",
    "    >> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', 'Bob', 'went', 'to', 'the', 'kitchen']\n",
    "    \"\"\"\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, sentence_maxlen, story_maxlen, query_maxlen, task):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    X_length = []\n",
    "    answers = []\n",
    "\n",
    "    prefix = task.split(\"_\")[0]\n",
    "    #print(\"REENTERING ROUTINE\")\n",
    "    fp = open (prefix + \"TESTstoriesplusqueries.txt\", \"a\")\n",
    "    #fp.write(task)\n",
    "    #fp.write(\"\\n\")\n",
    "    fq = open (prefix + \"TESTanswers.txt\", \"a\")\n",
    "    #fq.write(task)\n",
    "    #fq.write(\"\\n\")\n",
    "    i = 0\n",
    "    for story, query, answer in data:\n",
    "        sentences = []\n",
    "        longer = []\n",
    "        \n",
    "        \n",
    "        # Deal with answers\n",
    "#         print(\"answers\")\n",
    "#        print(answer)\n",
    "        answers = answer.split(\" \")\n",
    "        for answer in answers:\n",
    "            #fq.write(answer)\n",
    "            fq.write(\"{:d}\".format(word_idx[answer]))\n",
    "        fq.write(\"\\n\")\n",
    "        \n",
    "         #print(\"NEW STORY\")\n",
    "#         print(story)\n",
    "#         print(query)\n",
    "#        print(answer)\n",
    "#         print(longer)\n",
    "        for s in story:\n",
    "            sentence = [word_idx[w] for w in s]\n",
    "            longer.append(sentence)\n",
    "\n",
    "        question = [word_idx[w] for w in query]\n",
    "        \n",
    "        longer = [item for sublist in longer for item in sublist]\n",
    "        longer += question\n",
    "        \n",
    "#         print(\"prior try\")\n",
    "#         print(longer)\n",
    "        #print(\"The values being written are {}\".format(\",\".join(str(x) for x in longer)))\n",
    "        \n",
    "        yy = (x for x in longer)\n",
    "\n",
    "        xx = \",\".join(str(x) for x in longer)\n",
    "#         print(\"String\")\n",
    "#         print(xx)\n",
    "#         print(i, \"    \", longer)\n",
    "        for x in longer:\n",
    "            fp.write('%d ' % x)\n",
    "        fp.write(\"\\n\")\n",
    "        #fp.write(\"{}\\n\".format(\",\".join(str(x) for x in longer)))\n",
    "        i += 1\n",
    "    \n",
    "    fp.close()\n",
    "    fq.close()\n",
    "    X_length.append(len(sentences))        \n",
    "#     print(\"non string\")\n",
    "#     print(yy)\n",
    "\n",
    "    \n",
    "    return X, Xq, np.array(Y), X_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************\n",
      "\n",
      "qa1_single-supporting-fact\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa2_two-supporting-facts\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa3_three-supporting-facts\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa4_two-arg-relations\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa5_three-arg-relations\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa6_yes-no-questions\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa7_counting\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa8_lists-sets\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa9_simple-negation\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa10_indefinite-knowledge\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa11_basic-coreference\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa12_conjunction\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa13_compound-coreference\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa14_time-reasoning\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa15_basic-deduction\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa16_basic-induction\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa17_positional-reasoning\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa18_size-reasoning\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa19_path-finding\n",
      "\n",
      "**************************************\n",
      "\n",
      "qa20_agents-motivations\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# 'qa19_path-finding',\n",
    "tasks = [\n",
    "    'qa1_single-supporting-fact', 'qa2_two-supporting-facts', 'qa3_three-supporting-facts',\n",
    "    'qa4_two-arg-relations', 'qa5_three-arg-relations', 'qa6_yes-no-questions', 'qa7_counting',\n",
    "    'qa8_lists-sets', 'qa9_simple-negation', 'qa10_indefinite-knowledge',\n",
    "    'qa11_basic-coreference', 'qa12_conjunction', 'qa13_compound-coreference',\n",
    "    'qa14_time-reasoning', 'qa15_basic-deduction', 'qa16_basic-induction', 'qa17_positional-reasoning',\n",
    "    #qa18_size-reasoning',  'qa20_agents-motivations'\n",
    "    'qa18_size-reasoning',  'qa19_path-finding', 'qa20_agents-motivations'\n",
    "]\n",
    "\n",
    "verbose = True\n",
    "path = 'babi/babi_tasks_data_1_20_v1.2.tar.gz'\n",
    "tar = tarfile.open(path)\n",
    "tasks_dir = 'tasks_1-20_v1-2/en-10k/'\n",
    "\n",
    "i = 0\n",
    "\n",
    "for task in tasks:\n",
    "    print('\\n**************************************\\n')\n",
    "    print(task)\n",
    "    \n",
    "    task_path = tasks_dir + task + '_{}.txt'\n",
    "    train, test = get_data(task_path)\n",
    "    \n",
    "    prefix = str(task).split(\"_\")[0]\n",
    "    fr = open(prefix + \"TESTVocab.txt\", \"a\")\n",
    "              \n",
    "    flatten = lambda data: [i for i in chain(*data)]\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(flatten(story) + q + [answer])\n",
    "                                               for story, q, answer in train + test)))\n",
    "    \n",
    "#     print(\"\\n\\n\\n\")\n",
    "#     print(task)\n",
    "#     print(vocab)\n",
    "    #     #print(type(vocab))\n",
    "    for item in vocab:\n",
    "        #print(item)\n",
    "        fr.write(\"{}\\n\".format(item))\n",
    "\n",
    "\n",
    "    # 0 is reserved for masking via pad_sequences\n",
    "    vocab_size = len(vocab) + 1\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "#     print(\"\\n\")\n",
    "#     print(word_idx)\n",
    "\n",
    "    sentence_maxlen = max(flatten([[len(s) for s in x] for x, _, _ in train + test]))\n",
    "    #sentence_maxlen = max([i for i in chain(*[[len(s) for s in x] for x, _, _ in train + test])])\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "    \n",
    "    idx_word = {v: k for k, v in word_idx.iteritems()}\n",
    "    idx_word[0] = \"_PAD\"\n",
    "     \n",
    "    i += 1\n",
    "    #X, Xq, Y, X_length = vectorize_stories(train, word_idx, sentence_maxlen, story_maxlen, query_maxlen, task)\n",
    "    tX, tXq, tY, tX_length = vectorize_stories(test, word_idx, sentence_maxlen, story_maxlen, query_maxlen, task)\n",
    "    #print(word_idx)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "tasks = [\n",
    "    'qa1_single-supporting-fact', 'qa2_two-supporting-facts', 'qa3_three-supporting-facts',\n",
    "    'qa4_two-arg-relations', 'qa5_three-arg-relations', 'qa6_yes-no-questions', 'qa7_counting',\n",
    "    'qa8_lists-sets', 'qa9_simple-negation', 'qa10_indefinite-knowledge',\n",
    "    'qa11_basic-coreference', 'qa12_conjunction', 'qa13_compound-coreference',\n",
    "    'qa14_time-reasoning', 'qa15_basic-deduction', 'qa16_basic-induction', 'qa17_positional-reasoning',\n",
    "    'qa18_size-reasoning', 'qa19_path-finding', 'qa20_agents-motivations'\n",
    "]\n",
    "\n",
    "verbose = True\n",
    "path = 'babi/babi_tasks_data_1_20_v1.2.tar.gz'\n",
    "tar = tarfile.open(path)\n",
    "tasks_dir = 'tasks_1-20_v1-2/en/'\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "    print('\\n**************************************\\n')\n",
    "    print(task)\n",
    "    \n",
    "    task_path = tasks_dir + task + '_{}.txt'\n",
    "    train, test = get_data(task_path)\n",
    "    \n",
    "    flatten = lambda data: [i for i in chain(*data)]\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(flatten(story) + q + [answer])\n",
    "                                               for story, q, answer in train + test)))\n",
    "\n",
    "    # 0 is reserved for masking via pad_sequences\n",
    "    vocab_size = len(vocab) + 1\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "    sentence_maxlen = max(flatten([[len(s) for s in x] for x, _, _ in train + test]))\n",
    "    #sentence_maxlen = max([i for i in chain(*[[len(s) for s in x] for x, _, _ in train + test])])\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "    \n",
    "    idx_word = {v: k for k, v in word_idx.iteritems()}\n",
    "    idx_word[0] = \"_PAD\"\n",
    "     \n",
    "    X, Xq, Y, X_length = vectorize_stories(train, word_idx, sentence_maxlen, story_maxlen, query_maxlen)\n",
    "    tX, tXq, tY, tX_length = vectorize_stories(test, word_idx, sentence_maxlen, story_maxlen, query_maxlen)\n",
    "    \n",
    "    if verbose:\n",
    "        print('vocab = {}'.format(vocab))\n",
    "        print('word_idx = {}'.format(word_idx))\n",
    "        print('X.shape = {}'.format(np.array(X).shape))\n",
    "        print('Xq.shape = {}'.format(np.array(Xq).shape))\n",
    "        print('Y.shape = {}'.format(Y.shape))\n",
    "        print('story_maxlen, sentence_maxlen, query_maxlen = {}, {}, {}'.format(story_maxlen, sentence_maxlen, query_maxlen))\n",
    "\n",
    "    config = Config()\n",
    "    config.vocab_size = vocab_size\n",
    "    config.num_steps_sentence = sentence_maxlen\n",
    "    config.num_steps_story = story_maxlen\n",
    "    config.num_steps_question = query_maxlen\n",
    "    print('max_epochs = {}'.format(config.max_epochs)) \n",
    "    with tf.Graph().as_default() as g:\n",
    "        model = RNN_Model(config)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "\n",
    "            for epoch in range(config.max_epochs):\n",
    "                print('Epoch {}'.format(epoch))\n",
    "\n",
    "                train_loss, train_acc, val_acc, Story, Question, Answer, Prediction = model.run_epoch(session, (\n",
    "                    X, Xq, Y, X_length), train_op=model.train_step)\n",
    "\n",
    "                if verbose:\n",
    "                    print('Training loss: {}'.format(train_loss))\n",
    "                    print('Training acc: {}'.format(train_acc))\n",
    "                    print('Validation acc: {}'.format(val_acc))\n",
    "\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    test_acc = model.predict(session, (tX, tXq, tY, tX_length))\n",
    "                    print('Testing acc: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
